{
  "task": "entity",
  "test": false,
  "dict_directory": "./dicts/genia",
  "train_data": "./data/genia/passage_train.json",
  "test_data": "./data/genia/passage_test.json",

  "gpu": 1,
  "fine": false,
  "previous_model": {},
  "end_identifier": {
    "config_path": "./configs/genia/end/focal.json",
    "model_path": "./check_points/genia/focal/genia-end-focal-2022-05-17-17:52:52-best.pth",
    "train_coe": 4,
    "test_coe": 1.1,
    "extend": 2
  },

  "model": {
    "mode": "lstm",
    "direction": "backward",
   "encoder": {
      "bert": "./biobert-large-cased-v1.1-squad",
      "data": "genia",
      "train_word_embedding": true,
      "dim_bert": 1024,
      "dim_word": 200,
      "num_word": 14042,
      "dim_char": 256,
      "num_char": 84,
      "dim_char_lstm": 512,
      "num_char_layer": 2,
      "num_pos": 24,
      "dim_pos": 256,
      "dim_lstm": 1024,
      "num_lstm_layer": 2,
      "char_dropout": 0.5,
      "dropout": 0.5
    },
    "sub_sequence_encoder": {
      "dim_lstm": 1024,
      "num_layer": 1
    },
    "fc_layer": {
      "dim_in": 1024,
      "dim_hid": 512,
      "dim_out": 7,
      "dropout": 0.5
    }
  },

  "stages": ["train", "test"],

  "focal": false,
  "alpha": [1, 1, 1, 1, 1, 1, 1],
  "split_length": [50, 100, 150, 200, 250, 300, 400, 10000],
  "train_batch_size": [8, 6, 5, 4, 2, 1],
  "evaluate_batch_size": 20,
  "learning_rate": 1e-3,
  "bert_learning_rate": 1e-5,
  "saving_threshold": 81,

  "start_epoch": 1,
  "end_epoch": 100,
  "warmup_epoch": 5
}